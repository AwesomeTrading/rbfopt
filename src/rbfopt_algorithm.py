"""Main optimization algorithm.

This module contains a class that implements the main optimization
algorithm. The class has a state so that optimization can be stopped
and resumed at any time.

Licensed under Revised BSD license, see LICENSE.
(C) Copyright Singapore University of Technology and Design 2016.
(C) Copyright International Business Machines Corporation 2016.
Research partially supported by SUTD-MIT International Design Center.

"""

from __future__ import print_function
from __future__ import division
from __future__ import absolute_import

import sys
import math
import random
import time
import os
import pickle
import numpy as np
import rbfopt_utils as ru
import rbfopt_aux_problems as aux
import rbfopt_model_selection as ms
import rbfopt_config as config
from rbfopt_settings import RbfSettings

class OptAlgorithm:
    """Optimization algorithm.

    Implements the main optimization algorithm, and contains all its
    state variables so that the algorithm can be warm-started from a
    given state. Some of the logic of the algorithm is not inside this
    class, because it can be run in parallel and therefore should not
    modify the state.

    Parameters
    ----------

    settings : rbfopt_settings.RbfSettings
        Global and algorithmic settings.

    dimension : int
        The dimension of the problem, i.e. size of the space.

    var_lower : List[float]
        Vector of variable lower bounds.

    var_upper : List[float]
        Vector of variable upper bounds.

    objfun : Callable[List[float]]
        The unknown function we want to optimize.

    objfun_fast : Callable[List[float]]
        A faster, lower quality version of the unknown function we
        want to optimize. If None, it is assumed that such a version
        of the function is not available.

    integer_vars : List[int] or None
        A list containing the indices of the integrality constrained
        variables. If None or empty list, all variables are assumed to
        be continuous.

    init_node_pos : List[List[float]] or None
        Coordinates of points at which the function value is known. If
        None, the initial points will be generated by the
        algorithm. This must be of length at least dimension + 1, if
        provided.

    init_node_val : List[float] or None
        Function values corresponding to the points given in
        init_node_pos. Should be None if the previous argument is
        None.


    Attributes
    ----------

    elapsed_time : float
        Elapsed CPU time up to the point where state was last saved.

    best_local_rbf : string
        Best RBF type to construct model for local search.

    best_global_rbf : string
        Best RBF type to construct model for global search.

    n : int
        Dimension of the problem.

    itercount : int
        Iteration number.

    evalcount : int
        Total number of function evaluations in accurate mode.

    fast_evalcount : int
        Total number of function evaluations in fast mode.

    current_step : int
        Identifier of the current step within the cyclic optimization
        strategy counter.

    num_con_ls : int
        Current number of consecutive local searches.

    num_stalled_cycles : int
        Number of consecutive cycles without improvement.

    num_cons_discarded : int
        Number of consecutive discarded points.

    num_fast_restarts : int
        Number of restarts in fast mode.

    inf_step : int
        Identifier of the InfStep.

    local_search_step : int
        Identifier of the LocalSearchStep.

    cycle_length : int
        Length of an optimization cycle.

    restoration_step : int
        Identifier of the RestorationStep.

    first_step : int
        Identifier of the first step of an optimization  cycle.

    two_phase_optimization : bool
        Is the fast but noisy objective function is available?

    is_best_fast : bool
        Was the best known objective function evaluated in fast mode?

    current_mode : string
        Evaluation mode for the objective function at a given
        stage. Can be either 'fast' or 'accurate'.

    node_pos : List[List[float]]
        Coordinates of the interpolation nodes (i.e. the points where
        the objective function has already been evaluated). The
        coordinates may be in the transformed space. This list only
        includes points since the last restart.

    node_val : List[float]
        Objective function value at the points in node_pos. This list 
        only includes points since the last restart.

    node_is_fast : List[bool]
        For each interpolation node in node_pos, was it evaluated in
        'fast' mode?

    all_node_pos : List[List[float]]
        Coordinates of the interpolation nodes. This list contains all
        evaluated points in the original space, and is persistent
        across restarts.

    all_node_val : List[float]
        Objective function value at the points in all_node_pos.

    all_node_is_fast : List[bool]
        For each interpolation node in all_node_pos, was it evaluated
        in 'fast' mode?

    all_node_pos_size_at_restart : int
        Index of the first node in all_node_pos after the latest
        restart.

    l_lower : List[float]
        Variable lower bounds in the transformed space.

    l_upper : List[float]
        Variable upper bounds in the transformed space.

    fmin_index : int
        Index of the minimum value among the nodes.

    fmin : float
        Minimum value among the nodes.

    fmax : float
        Maximum value among the nodes.

    gap_den : float 
        Denominator of errormin.

    gap : float
        Minimum distance from the optimum.

    fmin_cycle_start : float
        Best value function at the beginning of the latest
        optimization cycle.

    """
    def __init__(self, settings, dimension, var_lower, var_upper,
                 objfun, objfun_fast = None, integer_vars = None,
                 init_node_pos = None, init_node_val = None):
        """Constructor.
        
        """

        assert(len(var_lower) == dimension)
        assert(len(var_upper) == dimension)
        assert((integer_vars is None) or (len(integer_vars) == 0) or
               (max(integer_vars) < dimension))
        assert(init_node_pos is None or 
               (len(init_node_pos) == len(init_node_val) and
                len(init_node_pos) >= dimension + 1))
        assert(isinstance(settings, RbfSettings))
        
        # Save references of initial data
        self.settings = settings
        self.var_lower = var_lower
        self.var_upper = var_upper
        self.objfun = objfun
        self.objfun_fast = objfun_fast
        self.integer_vars = integer_vars

        # Start timing
        self.elapsed_time = 0.0
        start_time = time.time()

        # Set the value of 'auto' parameters if necessary
        l_settings = settings.set_auto_parameters(dimension, var_lower,
                                                  var_upper, integer_vars)
        # Save references
        self.l_settings = l_settings

        # Local and global RBF models are usually the same
        self.best_local_rbf = l_settings.rbf
        self.best_global_rbf = l_settings.rbf
    
        # We use n to denote the dimension of the problem, same notation
        # of the paper. This is redundant but it simplifies our life.
        self.n = dimension

        # Set random seed. Some of the (external) libraries use numpy's
        # random generator, we use python's internal generator, so we have
        # to seed both for consistency.
        random.seed(l_settings.rand_seed)
        np.random.seed(l_settings.rand_seed)

        # Initialize counters
        self.itercount = 0
        self.evalcount = 0
        self.fast_evalcount = 0
        self.current_step = 0
        self.num_cons_ls = 0
        self.num_stalled_cycles = 0
        self.num_cons_discarded = 0
        self.num_fast_restarts = 0
    
        # Initialize identifiers of the search steps
        self.inf_step = 0
        self.local_search_step = (l_settings.num_global_searches + 1)
        self.cycle_length = (l_settings.num_global_searches + 2)
        self.restoration_step = (l_settings.num_global_searches + 3)
        # Determine which step is the first of each loop
        self.first_step = (self.inf_step if l_settings.do_infstep 
                           else self.inf_step + 1)

        # Initialize settings for two-phase optimization.
        if (objfun_fast is not None):
            self.two_phase_optimization = True
            self.is_best_fast = True
            self.current_mode = 'fast'
        else:
            self.two_phase_optimization = False
            self.is_best_fast = False
            self.current_mode = 'accurate'

        # Round variable bounds to integer if necessary
        ru.round_integer_bounds(var_lower, var_upper, integer_vars)

        # Save initial nodes
        self.init_node_pos = init_node_pos
        self.init_node_val = init_node_val
        # Initialize global lists
        self.all_node_pos, self.all_node_val = list(), list()
        self.node_pos, self.node_val = list(), list()
        # Store if each function evaluation is fast or accurate
        self.node_is_fast, self.all_node_is_fast = list(), list()
        # We need to remember the index of the first node in all_node_pos
        # after every restart
        self.all_node_pos_size_at_restart = 0

        # Update domain bounds if necessary
        (self.l_lower, 
         self.l_upper) = ru.transform_domain_bounds(l_settings, var_lower, 
                                                    var_upper)

        # Current minimum value among the nodes, and its index
        self.fmin_index = 0
        self.fmin = float('+inf')
        # Current maximum value among the nodes
        self.fmax = float('+inf')

        # Best value function at the beginning of an optimization cycle
        self.fmin_cycle_start = self.fmin

        # Set default output stream
        self.output_stream = sys.stdout

        # Update timer
        self.elapsed_time += time.time() - start_time

    # -- end function

    def set_output_stream(self, output_stream):
        """Set output stream for the log.

        Parameters
        ----------
        output_stream : file
            Stream to be used for output. Must have a 'write' and a
            'flush' method.
        """
        assert('write' in dir(output_stream))
        assert('flush' in dir(output_stream))
        self.output_stream = output_stream

    def update_log(self, tag, node_is_fast = None, obj_value = None, 
                   min_dist = None, gap = None):
        """Print a single line in the log.

        Update the program's log, writing information about an
        iteration of the optimization algorith, or a special message.

        Parameters
        ----------
        tag : string
            Iteration id tag, or unique message if at least one of the
            other arguments are None.

        node_is_fast : bool or None
            Is the objective function value to be printed associated
            with a node evaluated in fast mode?

        obj_value : float or None
            Objective function value to print.

        min_dist : float or None
            Euclidean distance to the closest node.

        gap : float or None
            Relative distance from the optimum. This will be
            multiplied by 100 before printing.

        """
        if (node_is_fast is None or obj_value is None or 
            min_dist is None or gap is None):
            print('Iteration {:3d}'.format(self.itercount) + 
                  ' {:s}'.format(tag), file = self.output_stream)
        else:
            print('Iteration {:3d}'.format(self.itercount) + 
                  ' {:16s}'.format(tag) +
                  ': objval{:s}'.format('~' if node_is_fast else ' ') +
                  ' {:16.6f}'.format(obj_value) +
                  ' min_dist {:9.4f}'.format(min_dist) +
                  ' gap {:8.2f}'.format(gap*100),
                  file = self.output_stream)
        self.output_stream.flush()
    # -- end function

    def print_summary_line(self, optimization_time, node_is_fast, gap):
        """Print summary line of the algorithm.
        
        Parameters
        ----------
        node_is_fast : bool
            Is the objective function value to be printed associated
            with a node evaluated in fast mode?

        gap : float
            Relative distance from the optimum. This will be
            multiplied by 100 before printing.
        """
        print('Summary: iters {:3d}'.format(self.itercount) + 
              ' evals {:3d}'.format(self.evalcount) + 
              ' fast_evals {:3d}'.format(self.fast_evalcount) + 
              ' opt_time {:7.2f}'.format(optimization_time) + 
              ' tot_time {:7.2f}'.format(self.elapsed_time) + 
              ' objval{:s}'.format('~' if node_is_fast else ' ') +
              ' {:15.6f}'.format(self.fmin) + 
              ' gap {:6.2f}'.format(100*gap),
              file = self.output_stream)
        self.output_stream.flush()
    # -- end function

    def save_to_file(self, filename):
        """Save object on file, with its state.

        Saves the current state of the algorithm on file. The
        optimization can be subsequently resumed reading the state
        from file. This function will also attempt to save the state
        of the random number generators so that if resumed on the same
        machine, the optimization process is identical to an
        uninterrupted process.

        Parameters
        ----------
        filename : string
            Name of the file that the state will be saved to.

        """
        # Save state of RNG
        self.random_state = random.getstate()
        self.np_random_state = np.random.get_state()
        # We cannot pickle these attributes. Erase them.
        objfun, objfun_fast = self.objfun, self.objfun_fast
        output_stream = self.output_stream
        self.objfun, self.objfun_fast, self.output_stream = None, None, None
        # Dump to file
        pickle.dump(self, open(filename, 'w'), pickle.HIGHEST_PROTOCOL)
        # Restore erased attributes
        self.objfun, self.objfun_fast = objfun, objfun_fast
        self.output_stream = output_stream
    # -- end function

    @classmethod
    def load_from_file(cls, filename, objfun, objfun_fast):
        """Load object from file, with its state.

        Read the current state from file, and return an object of this
        class. The optimization can be resumed immediately. This
        function will attempt to set the random number generators to
        the state they were in. Note that the output stream is set to
        stdout, regardless of the output stream when the state was
        saved, so the caller may have to set the desired output
        stream.

        Parameters
        ----------
        filename : string
            Name of the file from which the state will be read.

        objfun : Callable[List[float]]
            The unknown function we want to optimize. This cannot be
            read from file so it must be provided here.

        objfun_fast : Callable[List[float]]
            A faster, lower quality version of the unknown function we
            want to optimize. If None, it is assumed that such a
            version of the function is not available. This cannot be
            read from file so it must be provided here.
        
        Returns
        -------
        OptAlgorithm
            An object of this class.

        """
        assert(os.path.isfile(filename))
        alg = pickle.load(open(filename, 'r'))
        random.setstate(alg.random_state)
        np.random.set_state(alg.np_random_state)
        alg.objfun = objfun
        alg.objfun_fast = objfun_fast
        # Set default output stream
        alg.output_stream = sys.stdout
        return alg
    # -- end function

    def optimize(self, pause_after_iters = sys.maxsize):

        """Optimize a black-box function.

        Optimize an unknown function over a box using an RBF-based
        algorithm.

        Parameters
        ----------
        pause_after_iters : int
            Number of iterations after which the optimization process
            should pause. This allows the user to do other activities
            and resume optimization at a later time. Default
            sys.maxsize, which is larger than any practical integer.

        Returns
        -------
        (float, List[float], int, int, int)
            A quintuple (value, point, itercount, evalcount,
            fast_evalcount) containing the objective function value of
            the best solution found, the corresponding value of the
            decision variables, the number of iterations of the
            algorithm, the total number of function evaluations, and
            the number of these evaluations that were performed in
            'fast' mode.

        """
        start_time = time.time()
        
        # Localize variables that will be used often and that will
        # never be erased through the algorithm. These are all lists
        # or objects, so the reference points to the original.
        var_lower, var_upper = self.var_lower, self.var_upper
        l_lower, l_upper = self.l_lower, self.l_upper
        objfun, objfun_fast = self.objfun, self.objfun_fast
        integer_vars = self.integer_vars
        all_node_pos, all_node_val = self.all_node_pos, self.all_node_val
        all_node_is_fast = self.all_node_is_fast 
        settings, l_settings = self.settings, self.l_settings
        # The dimension will not be changed so it is safe to localize
        n = self.n

        # Save number of iterations at start
        itercount_at_start = self.itercount

        # If this is the first iteration, initialize the algorithm
        if (self.itercount == 0):
            self.restart()
            # We need to update the gap
            gap = ru.compute_gap(l_settings, self.fmin, self.is_best_fast)
            for (i, val) in enumerate(self.node_val):
                min_dist = ru.get_min_distance(self.node_pos[i], 
                                               self.node_pos[:i] + 
                                               self.node_pos[(i+1):])
                self.update_log('Initialization', self.node_is_fast[i], 
                                val, min_dist, gap)
        else: 
            gap = ru.compute_gap(l_settings, self.fmin, self.is_best_fast)

        # Main loop
        while (self.itercount - itercount_at_start < pause_after_iters and
               self.itercount < l_settings.max_iterations and
               self.evalcount < l_settings.max_evaluations and
               time.time() - start_time < l_settings.max_clock_time and
               gap > l_settings.eps_opt):
            # If the user wants to skip inf_step as in the original paper
            # of Gutmann (2001), we proceed to the next iteration.
            if (self.current_step == self.inf_step and 
                not l_settings.do_infstep):
                self.advance_step_counter()
                continue

            # Check if we should restart. We only restart if the initial
            # sampling strategy is random, otherwise it makes little sense.
            if ((self.num_cons_discarded >= 
                 l_settings.max_consecutive_discarded) or 
                (self.num_stalled_cycles >= l_settings.max_stalled_cycles and
                 self.evalcount + n + 1 < l_settings.max_evaluations and
                 l_settings.init_strategy != 'all_corners' and
                 l_settings.init_strategy != 'lower_corners')):

                self.update_log('Restart')
                self.restart(gap)

            # Number of nodes at current iteration
            k = len(self.node_pos)

            # Compute indices of fast node evaluations (sparse format)
            fast_node_index = ([i for (i, val) in enumerate(self.node_is_fast) 
                                if val] if self.two_phase_optimization 
                               else list())

            # If function scaling is automatic, determine which one to use
            if (settings.function_scaling == 'auto' and 
                self.current_step <= self.first_step):
                sorted_node_val = sorted(self.node_val)
                if (sorted_node_val[len(sorted_node_val)//2] - 
                    sorted_node_val[0] > l_settings.log_scaling_threshold):
                    l_settings.function_scaling = 'log'
                else:
                    l_settings.function_scaling = 'off'
        
            # Rescale nodes if necessary
            tfv = ru.transform_function_values(l_settings, self.node_val,
                                               self.fmin, self.fmax,
                                               fast_node_index)
            scaled_node_val, scaled_fmin, scaled_fmax, node_err_bounds = tfv

            # If RBF selection is automatic, at the beginning of each
            # cycle check if a different RBF yields a better model
            if (settings.rbf == 'auto' and k > n+1 and 
                self.current_step <= self.first_step):
                loc_iter = int(math.ceil(k*0.1))
                glob_iter = int(math.ceil(k*0.7))
                self.best_local_rbf = ms.get_best_rbf_model(l_settings, n, k, 
                                                            self.node_pos,
                                                            scaled_node_val,
                                                            loc_iter)
                self.best_global_rbf = ms.get_best_rbf_model(l_settings, n, k,
                                                             self.node_pos,
                                                             scaled_node_val,
                                                             glob_iter)
            # If we are in local search or just before local search, use a
            # local model.
            if (self.current_step >= (self.local_search_step - 1)):
                l_settings.rbf = self.best_local_rbf
            # Otherwise, global.
            else:
                l_settings.rbf = self.best_global_rbf

            try:
                # Compute the matrices necessary for the algorithm
                Amat = ru.get_rbf_matrix(l_settings, n, k, self.node_pos)
                Amatinv = ru.get_matrix_inverse(l_settings, Amat)

                # Compute RBF interpolant at current stage
                if (fast_node_index):
                    # Get coefficients for the exact RBF
                    rc = ru.get_rbf_coefficients(l_settings, n, k, 
                                                 Amat, scaled_node_val)
                    # RBF with some fast function evaluations
                    rc = aux.get_noisy_rbf_coefficients(l_settings, n, k, 
                                                        Amat[:k, :k],
                                                        Amat[:k, k:],
                                                        scaled_node_val,
                                                        fast_node_index,
                                                        node_err_bounds,
                                                        rc[0], rc[1])
                    (rbf_l, rbf_h) = rc
                else:
                    # Fully accurate RBF
                    rc = ru.get_rbf_coefficients(l_settings, n, k, 
                                                 Amat, scaled_node_val)
                    (rbf_l, rbf_h) = rc
                
            except np.linalg.LinAlgError:
                # Error in the solution of the linear system. We must
                # switch to a restoration phase.
                self.current_step = self.restoration_step
                self.node_val.pop()
                self.node_pos.pop()
                if (self.node_is_fast.pop()):
                    fast_node_index.pop()
                    tfv = ru.transform_function_values(l_settings, 
                                                       self.node_val,
                                                       self.fmin, self.fmax,
                                                       fast_node_index)
                    (scaled_node_val, scaled_fmin, scaled_fmax,
                     node_err_bounds) = tfv
                k = len(self.node_pos)


            # For displaying purposes, record what type of iteration
            # we are performing
            iteration_id = ''
        
            # Initialize the new point to None
            next_p = None

            if (self.current_step == self.inf_step):
                # Infstep: explore the parameter space
                next_p = pure_global_step(l_settings, n, k, l_lower,
                                          l_upper, self.node_pos, 
                                          Amatinv, integer_vars)
                iteration_id = 'InfStep'

            elif (self.current_step == self.restoration_step):
                # Restoration
                if (not self.restoration_step()):
                    self.update_log('Restoration phase failed. Abort.')
                    # This will force the optimization process to return
                    break
                iteration_id = 'Restoration'
            
            elif (self.current_step == self.local_search_step):
                # Local search
                (adj, next_p, 
                 ind) = local_step(l_settings, n, k, l_lower, l_upper,
                                   self.node_pos, rbf_l, rbf_h,
                                   integer_vars, tfv, fast_node_index,
                                   Amat, Amatinv, self.fmin_index,
                                   self.two_phase_optimization,
                                   self.current_mode, self.node_is_fast)

                # Re-evaluate point if necessary
                if (ind < len(self.node_pos)):
                    self.node_pos.pop(ind)
                    self.node_val.pop(ind)
                    self.node_is_fast.pop(ind)
                    all_node_pos.pop(all_node_pos_size_at_restart + ind)
                    all_node_val.pop(all_node_pos_size_at_restart + ind)
                    all_node_is_fast.pop(all_node_pos_size_at_restart + ind)
                    # We must update k here to make sure it is consistent
                    # until the start of the next iteration.
                    k = len(self.node_pos)
                if (adj):
                    iteration_id = 'AdjLocalStep'
                else:
                    iteration_id = 'LocalStep'                    

            else:
                # Global search
                next_p = global_step(l_settings, n, k, l_lower, l_upper,
                                     self.node_pos, rbf_l, rbf_h,
                                     integer_vars, tfv, Amatinv,
                                     self.fmin_index, self.current_step)
                iteration_id = 'GlobalStep'
            # -- end if
                                                     
            # If the optimization failed or the point is too close to
            # current nodes, discard it. Otherwise, add it to the list.
            if ((next_p is None) or 
                (ru.get_min_distance(next_p, self.node_pos) <= 
                 l_settings.min_dist)):
                self.advance_step_counter()
                self.num_cons_discarded += 1
                self.update_log('Discarded')
            else:
                min_dist = ru.get_min_distance(next_p, self.node_pos)
                # Transform back to original space if necessary
                next_p_orig = ru.transform_domain(l_settings, var_lower,
                                                  var_upper, next_p, True)
                # Evaluate the new point, in accurate mode or fast mode
                if (self.current_mode == 'accurate'):
                    next_val = objfun(next_p_orig)
                    self.evalcount += 1
                    self.node_is_fast.append(False)
                else: 
                    next_val = objfun_fast(next_p_orig)
                    self.fast_evalcount += 1
                    if (self.require_accurate_evaluation(next_val)):
                        self.update_log(iteration_id, True, next_val,
                                        min_dist, gap)
                        next_val = objfun(next_p_orig)
                        self.evalcount += 1
                        self.node_is_fast.append(False)
                    else:
                        self.node_is_fast.append(True)

                # Add to the lists
                self.node_pos.append(next_p)
                self.node_val.append(next_val)
                all_node_pos.append(next_p_orig)
                all_node_val.append(next_val)
                all_node_is_fast.append(self.node_is_fast[-1])

                self.advance_step_counter()
                self.num_cons_discarded = 0
                        
                # Update fmin
                if (next_val < self.fmin):
                    self.fmin_index = k
                    self.fmin = next_val
                    self.is_best_fast = self.node_is_fast[-1]
                self.fmax = max(self.fmax, next_val)
                gap = min(ru.compute_gap(l_settings, next_val, 
                                         self.is_best_fast), gap)
                self.update_log(iteration_id, self.node_is_fast[-1], 
                                next_val, min_dist, gap)

            # Update iteration number
            self.itercount += 1

            # At the beginning of each loop of the cyclic optimization
            # strategy, check if the main loop is stalling
            self.stalling_update()

            # Check if we should switch to the second phase of
            # two-phase optimization.
            self.phase_update()
            
        # -- end while

        # Find best point and return it
        i = all_node_val.index(min(all_node_val))
        self.fmin = all_node_val[i]
        gap = ru.compute_gap(l_settings, self.fmin, self.all_node_is_fast[i])
        # Update timer
        self.elapsed_time += time.time() - start_time

        # Print summary and return
        self.print_summary_line(time.time() - start_time,
                                all_node_is_fast[i], gap)
        return (all_node_val[i], all_node_pos[i],
                self.itercount, self.evalcount, self.fast_evalcount)
    # -- end function

    def restart(self, current_gap = float('inf')):
        """Perform a complete restart of the optimization.

        Restart the optimization algorithm, i.e. discard the current
        RBF model, and select new sample points to start the algorithm
        from scratch. Previous point evaluations are ignored, but they
        are still recorded in the appropriate arrays.

        Parameters
        ----------
        current_gap : float
            The current optimality gap. This is used purely for
            visualization purposes in the log file, but does not
            affect the behavior of the function in other ways.

        """
        # We update the number of fast restarts here, so that if
        # we hit the limit on fast restarts, we can evaluate
        # points in accurate mode after restarting (even if
        # current_mode is updated in a subsequent block of code)
        self.num_fast_restarts += (1 if self.current_mode == 'fast' 
                                   else 0)
        # Store the current number of nodes
        self.all_node_pos_size_at_restart = len(self.all_node_pos)
        if (self.itercount > 0 or self.init_node_pos is None):
            # Compute a new set of starting points
            node_pos = ru.initialize_nodes(self.l_settings, self.var_lower, 
                                           self.var_upper, self.integer_vars)
            if (self.current_mode == 'accurate' or
                self.num_fast_restarts > self.l_settings.max_fast_restarts or
                (self.fast_evalcount + self.n + 1 >=
                 self.l_settings.max_fast_evaluations)):
                node_val = [self.objfun(point) for point in node_pos]
                self.evalcount += len(node_val)
            else:
                node_val = [self.objfun_fast(point) for point in node_pos]
                self.fast_evalcount += len(node_val)
            self.node_is_fast = [self.current_mode == 'fast' 
                                 for val in node_val]
        else:
            node_pos, node_val = self.init_node_pos, self.init_node_val
            # Initial points provided by the user are 'accurate'.
            self.node_is_fast = [False for val in node_val]

        self.all_node_pos.extend(node_pos)
        self.all_node_val.extend(node_val)
        self.all_node_is_fast.extend(self.node_is_fast)

        # Rescale the domain of the function
        node_pos = [ru.transform_domain(self.l_settings, self.var_lower,
                                        self.var_upper, point)
                    for point in node_pos]           
        # Update references
        self.node_pos, self.node_val = node_pos, node_val
        # Update all counters and values to restart properly
        self.fmin_index = node_val.index(min(node_val))
        self.fmin = node_val[self.fmin_index]
        self.fmax = max(node_val)
        self.fmin_cycle_start = self.fmin
        self.num_stalled_cycles = 0
        self.num_cons_discarded = 0
        self.is_best_fast = self.node_is_fast[self.fmin_index]

        gap  = min(ru.compute_gap(self.l_settings, self.fmin,
                                  self.is_best_fast), current_gap)
        # Print the initialization points
        for (i, val) in enumerate(self.node_val):
            min_dist = ru.get_min_distance(self.node_pos[i], 
                                           self.node_pos[:i] + 
                                           self.node_pos[(i+1):])
            self.update_log('Initialization', self.node_is_fast[i],
                            val, min_dist, gap)

    # -- end function

    def restoration_step(self):
        """Perform restoration step to repair RBF matrix.

        Try to repair an ill-conditioned RBF matrix by selecting
        random points far enough from current interpolation nodes,
        until numerical stability is restored.

        Returns
        -------
        bool
            Indicates whether the matrix was restored or not.

        """
        restoration_done = False
        cons_restoration = 0
        Amat = None
        Amatinv = None
        while (not restoration_done and cons_restoration < 
               self.l_settings.max_consecutive_restoration):
            next_p = [random.uniform(self.var_lower[i], self.var_upper[i])
                      for i in range(self.n)]
            ru.round_integer_vars(next_p, self.integer_vars)
            if (ru.get_min_distance(next_p, self.node_pos) >
                self.l_settings.min_dist):
                # Try inverting the RBF matrix to see if
                # nonsingularity is restored
                try:
                    Amat = ru.get_rbf_matrix(self.l_settings, self.n, 
                                             len(self.node_pos) + 1,
                                             self.node_pos + [next_p])
                    Amatinv = ru.get_matrix_inverse(self.l_settings, Amat)
                    restoration_done = True
                except np.linalg.LinAlgError:
                    cons_restoration += 1
            else:
                cons_restoration += 1
        return restoration_done
    # -- end function

    def phase_update(self):
        """Check if we should switch phase in two-phase optimization.

        Check if we should switch to the second phase of two-phase
        optimization. The conditions for switching are:
        1) Optimization in fast mode restarted too many times.
        2) We reached the limit of fast mode iterations.
        If both are met, the switch is performed.
        """
        if ((self.two_phase_optimization == True) and 
            (self.current_mode == 'fast') and
            ((self.num_fast_restarts > self.l_settings.max_fast_restarts) or
             (self.itercount >= self.l_settings.max_fast_iterations) or
             (self.fast_evalcount >= self.l_settings.max_fast_evaluations))):
            self.update_log('Switching to accurate mode')
            self.current_mode = 'accurate'            
    # -- end function

    def advance_step_counter(self):
        """Advance the step counter of the optimization algorithm.
        
        Determine the next search step of the optimization algorithm,
        and proceed to it.
        """
        # If we are in local search and there has been sufficient
        # improvement, repeat it.
        if ((self.current_step == self.local_search_step) and
            (self.node_val[-1] <= self.fmin -
             self.l_settings.eps_impr*max(1.0,abs(self.fmin))) and
            (self.num_cons_ls < 
             self.l_settings.max_consecutive_local_searches - 1)):
            self.num_cons_ls += 1
        # Otherwise, advance to next step.
        else:            
            self.current_step = ((self.current_step + 1) % 
                                 self.cycle_length)
            self.num_cons_ls = 0                    
    # -- end function

    def stalling_update(self):
        """Check if the algorithm is stalling.

        Check if the algorithm is stalling, and perform the
        corresponding updates.
        """
        if (self.current_step <= self.first_step): 
            if (self.fmin <= (self.fmin_cycle_start - 
                              self.l_settings.max_stalled_objfun_impr
                              * max(1.0, abs(self.fmin_cycle_start)))):
                self.num_stalled_cycles = 0
                self.fmin_cycle_start = self.fmin
            else:
                self.num_stalled_cycles += 1
    # -- end function

    def require_accurate_evaluation(self, fast_val):
        """Check if a given fast value qualifies for accurate evaluation.

        Verify if a point with the given objective function value in
        fast mode qualifies for an immediate accurate re-evaluation.

        Parameters
        ----------
        fast_val : float
            Value of the point to be tested, in fast mode.

        Returns
        -------
        bool
            True if the point should be re-evaluated in accurate mode
            immediately.
        """
        # Check if the point improves over existing points, 
        # or if it could be optimal according to tolerances. 
        # In this case, perform a double evaluation.
        best_possible = ((ru.get_fast_error_bounds(self.l_settings,
                                                   self.fmin)[0]
                          if self.is_best_fast else 0.0) +  self.fmin)
        if ((fast_val <= best_possible -
             self.l_settings.eps_impr*max(1.0, abs(best_possible))) or
            (fast_val <= self.l_settings.target_objval +
             self.l_settings.eps_opt*abs(self.l_settings.target_objval) -
             ru.get_fast_error_bounds(self.l_settings, fast_val)[0])):
            return True
        else:
            return False
    # -- end function
# -- end class

def pure_global_step(settings, n, k, var_lower, var_upper,
                     node_pos, mat, integer_vars):
    """Perform the pure global search step.
    
    Parameters
    ----------
    mat : numpy.matrix
        The matrix necessary for the computation. This is the inverse
        of the matrix [Phi P; P^T 0]. Must be a square numpy.matrix of
        appropriate dimension.

    settings : rbfopt_settings.RbfSettings
        Global and algorithmic settings.

    n : int
        The dimension of the problem, i.e. size of the space.

    k : int
        Number of nodes, i.e. interpolation points.

    var_lower : List[float]
        Vector of variable lower bounds.
    
    var_upper : List[float]
        Vector of variable upper bounds.

    node_pos : List[List[float]]
        List of coordinates of the nodes

    mat : numpy.matrix
        The matrix necessary for the computation. This is the inverse
        of the matrix [Phi P; P^T 0], see paper as cited above. Must
        be a square numpy.matrix of appropriate dimension.

    integer_vars : List[int] or None
        A list containing the indices of the integrality constrained
        variables. If None or empty list, all variables are assumed to
        be continuous.

    Returns
    -------
    List[float] or None
        The point to be evaluated next, or None if errors occurred.
    """
    assert(len(var_lower)==n)
    assert(len(var_upper)==n)
    assert(len(node_pos)==k)
    assert(isinstance(mat, np.matrix))
    assert(isinstance(settings, RbfSettings))
    # Infstep: explore the parameter space
    return aux.pure_global_search(settings, n, k, var_lower, var_upper, 
                                  node_pos, mat, integer_vars)
# -- end function

def local_step(settings, n, k, var_lower, var_upper, node_pos,
               rbf_lambda, rbf_h, integer_vars, tfv, fast_node_index,
               Amat, Amatinv, fmin_index, two_phase_optimization, 
               current_mode, node_is_fast):
    """Perform local search step, possibly adjusted.
    
    Perform a local search step. This typically accepts the
    minimum of the RBF model as the next point if it is a viable
    option; if this is not viable, it will perform an adjusted
    local search and try to generate a different candidate. It
    also verifies if it is better to evaluate a brand new point,
    or re-evaluate a previously known point. The test is based on
    bumpiness of the resulting interpolant.
    
    Parameters
    ----------
    settings : rbfopt_settings.RbfSettings
        Global and algorithmic settings.

    n : int
        The dimension of the problem, i.e. size of the space.

    k : int
        Number of nodes, i.e. interpolation points.

    var_lower : List[float]
        Vector of variable lower bounds.

    var_upper : List[float]
        Vector of variable upper bounds.

    node_pos : List[List[float]]
        List of coordinates of the nodes.

    rbf_lambda : List[float]
        The lambda coefficients of the RBF interpolant, corresponding
        to the radial basis functions. List of dimension k.

    rbf_h : List[float]
        The h coefficients of the RBF interpolant, corresponding to
        the polynomial. List of dimension n+1.

    integer_vars: List[int] or None
        A list containing the indices of the integrality constrained
        variables. If None or empty list, all variables are assumed to
        be continuous.

    tfv : (List[float], float, float, List[(float, float)])
        Transformed function values: scaled node values, scaled
        minimum, scaled maximum, and node error bounds.

    fast_node_index : List[int]
        List of indices of nodes whose function value should be
        considered variable withing the allowed range.

    Amat : numpy.matrix
        RBF matrix, i.e. [Phi P; P^T 0].

    Amatinv : numpy.matrix
        Inverse of the RBF matrix, i.e. [Phi P; P^T 0]^{-1}.

    fmin_index : int
        Index of the minimum value among the nodes.

    two_phase_optimization : bool
        Is the fast but noisy objective function is available?

    current_mode : string
        Evaluation mode for the objective function at a given
        stage. Can be either 'fast' or 'accurate'.

    node_is_fast : List[bool]
        For each interpolation node in node_pos, was it evaluated in
        'fast' mode?

    Returns
    -------
    (bool, List[float] or None, int)
        A triple (adjusted, point, index) where adjusted is True if
        the local search was adjusted rather than a pure local search,
        point is the point to be evaluated next (or None if errors
        occurred), and int is the insertion index for the
        point. Typically, the insertion index will be equal to the
        length of node_pos, but it may be a smaller index in case a
        point previously evaluated in fast mode needs to be
        re-evaluated in accurate mode.

    See also
    --------
    rbfopt_utils.transform_function_values()

    """
    assert(len(var_lower)==n)
    assert(len(var_upper)==n)
    assert(len(rbf_lambda)==k)
    assert(len(node_pos)==k)
    assert(len(node_is_fast)==k)
    assert(0 <= fmin_index < k)
    assert((current_mode=='fast') or (current_mode=='accurate'))
    assert(isinstance(settings, RbfSettings))
    assert(isinstance(Amat, np.matrix))
    assert(isinstance(Amatinv, np.matrix))
    scaled_node_val, scaled_fmin, scaled_fmax, node_err_bounds = tfv
    # Local search: compute the minimum of the RBF.
    min_rbf = aux.minimize_rbf(settings, n, k, var_lower, var_upper,
                               node_pos, rbf_lambda, rbf_h, integer_vars)
    if (min_rbf is not None):
        min_rbf_val = ru.evaluate_rbf(settings, min_rbf, n, k, 
                                      node_pos, rbf_lambda, rbf_h)
    # If the RBF cannot me minimized, or if the minimum is
    # larger than the node with smallest value, just take the
    # node with the smallest value.
    if (min_rbf is None or 
        (min_rbf_val >= scaled_fmin + settings.eps_zero)):
        min_rbf = node_pos[fmin_index]
        min_rbf_val = scaled_fmin
    # Check if point can be accepted: is there an improvement?
    if (min_rbf_val <= (scaled_fmin - settings.eps_impr * 
                        max(1.0, abs(scaled_fmin)))):
        target_val = min_rbf_val
        next_p = min_rbf
        adjusted = False
    else:
        # If the point is not improving, we solve a global
        # search problem, rescaling the search box to enforce
        # some sort of local search
        target_val = scaled_fmin - 0.01*max(1.0, abs(scaled_fmin))
        dist_weight = 0.05
        local_varl = [max(var_lower[i], min_rbf[i] -
                          settings.local_search_box_scaling * 
                          0.33 * (var_upper[i] - var_lower[i]))
                      for i in range(n)]
        local_varu = [min(var_upper[i], min_rbf[i] +
                          settings.local_search_box_scaling * 
                          0.33 * (var_upper[i] - var_lower[i]))
                      for i in range(n)]
        ru.round_integer_bounds(local_varl, local_varu, 
                                integer_vars)
        next_p  = aux.global_search(settings, n, k, 
                                    local_varl, local_varu, 
                                    node_pos, rbf_lambda, 
                                    rbf_h, Amatinv, target_val,
                                    dist_weight, integer_vars)
        adjusted = True
        
    # If previous points were evaluated in low quality and we are
    # now in high-quality local search mode, then we should verify
    # if it is better to evaluate a brand new point or re-evaluate
    # a previously known point.
    if ((two_phase_optimization == True) and (current_mode == 'accurate')):
        (ind, bump) = ru.get_min_bump_node(settings, n, k, Amat, 
                                           scaled_node_val, fast_node_index, 
                                           node_err_bounds, target_val)
        
        if (ind is not None and next_p is not None):
            # Check if the newly proposed point is very close to an
            # existing one.
            if (ru.get_min_distance(next_p, node_pos) > settings.min_dist):
                # If not, compute bumpiness of the newly proposed point.
                n_bump = ru.get_bump_new_node(settings, n, k, node_pos,
                                              scaled_node_val, next_p,
                                              fast_node_index, 
                                              node_err_bounds,
                                              target_val)
            else:
                # If yes, we will simply reevaluate the existing point
                # (if it can be reevaluated).
                ind = ru.get_min_distance_index(next_p, node_pos)
                n_bump = (float('inf') if node_is_fast[ind] else
                          float('-inf'))
            if (n_bump > bump):
                # In this case we want to put the new point at the
                # same location as one of the old points.
                return (True, node_pos[ind], ind)
        # -- end if
    # -- end if
    return (adjusted, next_p, k)
# -- end function

def global_step(settings, n, k, var_lower, var_upper, node_pos,
                rbf_lambda, rbf_h, integer_vars, tfv, Amatinv,
                fmin_index, current_step):
    """Perform global search step.
    
    Perform a global search step, with a different methodology
    depending on the algorithm chosen.
        
    Parameters
    ----------
    settings : rbfopt_settings.RbfSettings
        Global and algorithmic settings.

    n : int
        The dimension of the problem, i.e. size of the space.

    k : int
        Number of nodes, i.e. interpolation points.

    var_lower : List[float]
        Vector of variable lower bounds.

    var_upper : List[float]
        Vector of variable upper bounds.

    node_pos : List[List[float]]
        List of coordinates of the nodes.

    rbf_lambda : List[float]
        The lambda coefficients of the RBF interpolant, corresponding
        to the radial basis functions. List of dimension k.

    rbf_h : List[float]
        The h coefficients of the RBF interpolant, corresponding to
        the polynomial. List of dimension n+1.

    integer_vars: List[int] or None
        A list containing the indices of the integrality constrained
        variables. If None or empty list, all variables are assumed to
        be continuous.

    tfv : (List[float], float, float, List[(float, float)])
        Transformed function values: scaled node values, scaled
        minimum, scaled maximum, and node error bounds.

    Amatinv : numpy.matrix
        The matrix necessary for the computation. This is the inverse
        of the matrix [Phi P; P^T 0], see paper as cited above. Must
        be a square numpy.matrix of appropriate dimension.

    fmin_index : int
        Index of the minimum value among the nodes.

    current_step : int
        Identifier of the current step within the cyclic optimization
        strategy counter.

    Returns
    -------
    List[float] or None
        The point to be evaluated next, or None if errors occurred.

    """
    assert(len(var_lower)==n)
    assert(len(var_upper)==n)
    assert(len(rbf_lambda)==k)
    assert(len(node_pos)==k)
    assert(0 <= fmin_index < k)
    assert(isinstance(Amatinv, np.matrix))
    assert(isinstance(settings, RbfSettings))
    assert(0 <= current_step <= settings.num_global_searches)

    scaled_node_val, scaled_fmin, scaled_fmax, node_err_bounds = tfv

    # Global search: compromise between finding a good value of the
    # objective function, and improving the model.
    if (settings.algorithm == 'Gutmann'):
        # If we use Gutmann's algorithm, we need the minimum of the
        # RBF interpolant to choose the target value.
        min_rbf = aux.minimize_rbf(settings, n, k, var_lower, var_upper, 
                                   node_pos, rbf_lambda, rbf_h, integer_vars)
        if (min_rbf is not None):
            min_rbf_val = ru.evaluate_rbf(settings, min_rbf, n, k,
                                          node_pos, rbf_lambda, rbf_h)
        # If the RBF cannot me minimized, or if the minimum is larger
        # than the node with smallest value, just take the node with
        # the smallest value.
        if (min_rbf is None or 
            min_rbf_val >= scaled_fmin + settings.eps_zero):
            min_rbf = node_pos[fmin_index]
            min_rbf_val = scaled_fmin
    else:
        # For the Metric SRSM method, pick the node with the smallest
        # value as minimum of the RBF.
        min_rbf = node_pos[fmin_index]
        min_rbf_val = scaled_fmin
        
    # Compute the function value used to determine the target
    # value. This is given by the sorted value in position sigma_n,
    # where sigma_n is a function described in the paper by Gutmann
    # (2001). If clipping is disabled, we simply take the largest
    # function value.
    if (settings.targetval_clipping):
        local_fmax = ru.get_fmax_current_iter(settings, n, k,
                                              current_step, scaled_node_val)
    else:
        local_fmax = scaled_fmax
                
    # For Gutmann's RBF method, the scaling factor is 1 - h/kappa,
    # where h goes from 0 to kappa-1 over the course of one global
    # search cycle, and kappa is the number of global searches.
    scaling = (1 - ((current_step - 1) / settings.num_global_searches))**2
    target_val = (min_rbf_val - scaling * (local_fmax - min_rbf_val))
    # For Metric SRSM, The weighting factor for the distance is is
    # (h+1)/kappa, where h goes from 0 to kappa-1 over the course of
    # one global search cycle, and kappa is the number of global
    # searches. If dist_weight would be 0, we set it to 0.05.
    dist_weight = (1 - (current_step / settings.num_global_searches)
                   if (current_step < settings.num_global_searches)
                   else 0.05)

    # If the global search is almost a local search, we restrict the
    # search to a box following Regis and Shoemaker (2007)
    if (scaling <= config.LOCAL_SEARCH_THRESHOLD):
        local_varl = [max(var_lower[i], min_rbf[i] -
                          settings.local_search_box_scaling * 
                          math.sqrt(scaling) * (var_upper[i] - var_lower[i]))
                      for i in range(n)]
        local_varu = [min(var_upper[i], min_rbf[i] +
                          settings.local_search_box_scaling * 
                          math.sqrt(scaling) * (var_upper[i] - var_lower[i]))
                      for i in range(n)]
        ru.round_integer_bounds(local_varl, local_varu, integer_vars)
    else:
        # Otherwise, use original bounds
        local_varl = var_lower
        local_varu = var_upper

    return aux.global_search(settings, n, k, local_varl, local_varu,
                             node_pos, rbf_lambda, rbf_h, Amatinv,
                             target_val, dist_weight, integer_vars)
# -- end function

